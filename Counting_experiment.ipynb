{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for the experiment\n",
    "By Loes (and Merel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'spectograms/'\n",
    "label_csv = pd.read_csv(\"secdata.csv\")\n",
    "names = label_csv[\"Filename\"].tolist()\n",
    "\n",
    "images = []\n",
    "count = 0\n",
    "for i in range(len(names)):\n",
    "    name = str(directory+names[i][:-4]+\".png\")\n",
    "    try:\n",
    "        im = Image.open(name).convert(\"RGB\")\n",
    "        images.append(np.asarray(im))\n",
    "    except Exeption:\n",
    "        print(\"something went wrong!!\")\n",
    "        print(\"with name = \", names[i])\n",
    "        print(\"and index i =\", i)\n",
    "        count += 1\n",
    "        \n",
    "print(count)\n",
    "if(count > 0):\n",
    "    print(\"Something went wrong!!! Please check it out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do not use the data you do not need\n",
    "for i in reversed(range(len(label_csv))):\n",
    "    if(label_csv.iloc[i][\"Speakers\"] > 5):\n",
    "        del images[i]\n",
    "        label_csv = label_csv.drop(label_csv.index[i])\n",
    "        \n",
    "print(len(images))\n",
    "print(len(label_csv))\n",
    "\n",
    "images = images[0:3000]\n",
    "labels = label_csv[\"Speakers\"].head(3000).tolist()\n",
    "\n",
    "print(len(images))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(labels, return_counts=True) #Count total amount of files per label\n",
    "print(unique, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data here, split in 3 parts. 0.7 training, 0.2 validation, 0.1 testing is used for large datasets\n",
    "x_train, x_val, x_test = np.split(images, [int(.7*len(labels)), int(.9*len(labels))])\n",
    "y_train, y_val, y_test = np.split(labels, [int(.7*len(labels)), int(.9*len(labels))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#IF YOU WANT TO UNDERSAMPLE THE TRAINING SET \n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], 288*432*3)) #for undersampling\n",
    "\n",
    "undersample = RandomUnderSampler(sampling_strategy='not minority')\n",
    "xtrainsample, ytrainsample = undersample.fit_resample(x_train, y_train)\n",
    "\n",
    "xtrainsample = np.reshape(xtrainsample, (xtrainsample.shape[0], 288, 432, 3))\n",
    "\n",
    "unique, counts = np.unique(ytrainsample, return_counts=True) # count everything in the labels for the report :) \n",
    "print(unique, counts)\n",
    "\n",
    "x_train = np.float(xtrainsample)\n",
    "y_train = np.float(ytrainsample)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.float32(y_test)\n",
    "y_val = np.float32(y_val)\n",
    "x_train = np.float32(x_train)\n",
    "x_test = np.float32(x_test)\n",
    "x_val = np.float32(x_val)\n",
    "y_train = np.float32(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute class weights to balance data\n",
    "# Scaling by total/5 helps keep the loss to a similar magnitude.\n",
    "# The sum of the weights of all examples stays the same.\n",
    "# needs to be done for each class! \n",
    "unique, counts = np.unique(y_train, return_counts=True) # only balance train data\n",
    "print(unique, counts)\n",
    "\n",
    "total = counts[0]+counts[1] + counts[2] + counts[3] + counts[4] \n",
    "weight_for_1 = (1 / counts[0])*(total)/5.0 \n",
    "weight_for_2 = (1 / counts[1])*(total)/5.0\n",
    "weight_for_3 = (1 / counts[2])*(total)/5.0\n",
    "weight_for_4 = (1 / counts[3])*(total)/5.0\n",
    "weight_for_5 = (1 / counts[4])*(total)/5.0\n",
    "class_weight = {1: weight_for_1, 2: weight_for_2, 3: weight_for_3, 4: weight_for_4, 5: weight_for_5}\n",
    "print(class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating one-hot notations\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=6)\n",
    "y_val = tf.keras.utils.to_categorical(y_val, num_classes=6)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specshape=(288, 432, 3) #shape of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports for the neural networks\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Conv2D, Conv1D, MaxPooling2D, MaxPooling1D, Dropout, LSTM, Dense, TimeDistributed, Flatten, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The CountNet model \n",
    "# aka model A\n",
    "def build_model_countnet():\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=1.)\n",
    "    \n",
    "    model.add(Conv2D(filters = 64, kernel_size=(3,3), activation='relu',kernel_initializer=initializer, input_shape=specshape))\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu', kernel_initializer=initializer))\n",
    "    model.add(MaxPooling2D(pool_size=(3,3)))\n",
    "    \n",
    "    model.add(Conv2D(filters =128, kernel_size=(3,3), activation='relu', kernel_initializer=initializer))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu', kernel_initializer=initializer))\n",
    "    model.add(MaxPooling2D(pool_size=(3,3)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    \n",
    "    model.add(LSTM(40, return_sequences=True))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "   \n",
    "    model.add(Flatten()) #added this...\n",
    "    model.add(Dense(6, activation='softmax'))\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/hamzag95/keras/blob/master/examples/cifar10_cnn.py\n",
    "#aka TowardsDataScience model \n",
    "#aka model B\n",
    "def build_model_cifar10():\n",
    "    \n",
    "    initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=1.)\n",
    "    \n",
    "    input_layer = tf.keras.Input(shape=specshape)\n",
    "    conv1 = tf.keras.layers.Conv2D(32, kernel_size=(3,3), activation=\"relu\", kernel_initializer=initializer)(input_layer)\n",
    "    conv2 = tf.keras.layers.Conv2D(32, kernel_size=(3,3), activation=\"relu\", kernel_initializer=initializer)(conv1)\n",
    "    max1 = tf.keras.layers.MaxPooling2D(pool_size=(2,2))(conv2)\n",
    "    drop1 = tf.keras.layers.Dropout(0.25)(max1)\n",
    "    \n",
    "    conv3 = tf.keras.layers.Conv2D(64, kernel_size=(3,3), activation=\"relu\", kernel_initializer=initializer)(drop1)\n",
    "    conv4 = tf.keras.layers.Conv2D(64, kernel_size=(3,3), activation=\"relu\", kernel_initializer=initializer)(conv3)\n",
    "    max2 = tf.keras.layers.MaxPooling2D(pool_size=(2,2))(conv4)\n",
    "    drop2 = tf.keras.layers.Dropout(0.25)(max2) \n",
    "    \n",
    "    flat1 = tf.keras.layers.Flatten()(drop2)\n",
    "    dense1 = tf.keras.layers.Dense(256, activation=\"relu\", kernel_initializer=initializer)(flat1) #originally 512\n",
    "    drop3 = tf.keras.layers.Dropout(0.25)(dense1) #originally 0.5\n",
    "    output_layer = tf.keras.layers.Dense(6, activation=\"softmax\", kernel_initializer=initializer)(drop3)\n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aka model C\n",
    "def build_model_andrei():     \n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=1.)\n",
    "    model.add(Conv2D(filters = 32, kernel_size=(8,8), activation='relu',kernel_initializer=initializer, input_shape=specshape))\n",
    "    model.add(MaxPooling2D(pool_size=(1,1)))\n",
    "    model.add(Conv2D(filters = 64, kernel_size=(6,6), activation='relu',kernel_initializer=initializer))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(filters = 128, kernel_size=(4,4), activation='relu',kernel_initializer=initializer))\n",
    "    model.add(MaxPooling2D(pool_size=(1,1)))\n",
    "    \n",
    "    #not using the 2 extra conv blocks..\n",
    "    \n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Dense(128, activation='relu',kernel_initializer=initializer)) #originally 1024\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(64, activation='relu',kernel_initializer=initializer)) #originally 512\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(32, activation='relu',kernel_initializer=initializer)) #originally 256\n",
    "    model.add(Dropout(0.1)) #originally 0.5\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(6, activation='softmax', kernel_initializer=initializer))\n",
    "   \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = build_model_andrei()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001), loss=tf.keras.losses.categorical_crossentropy, metrics=['accuracy'])#, class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Callbacks\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
    "                              patience=0, min_lr=0.00001)\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto',\n",
    "    baseline=None, restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the confusionmatrix after each epoch\n",
    "from sklearn.metrics import confusion_matrix as cm\n",
    "\n",
    "class ConfusionMatrix(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "    model = []\n",
    "    \n",
    "    def setup(self, model, validation_x, validation_y):\n",
    "        self.model = model\n",
    "        self.x = validation_x\n",
    "        self.y = validation_y\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        print(\"Confusion matrix\")\n",
    "        y_prob = self.model.predict(self.x)\n",
    "        y_pred = np.argmax(y_prob, axis=1)\n",
    "        y_true = np.argmax(self.y, axis=1)\n",
    "        print(cm(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_val = ConfusionMatrix()\n",
    "cm_val.setup(model, x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Print the counts for train, validation and test set again\n",
    "unique, counts = np.unique(y_train.argmax(axis=1), return_counts=True) # only balance train data! \n",
    "print('train set = ')\n",
    "print(unique, counts)\n",
    "\n",
    "unique, counts = np.unique(y_val.argmax(axis=1), return_counts=True) # only balance train data! \n",
    "print('val set = ')\n",
    "print(unique, counts)\n",
    "\n",
    "unique, counts = np.unique(y_test.argmax(axis=1), return_counts=True) # only balance train data! \n",
    "print('test set = ')\n",
    "print(unique, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, batch_size=16, epochs=10, validation_data=(x_val, y_val), callbacks=[reduce_lr, early_stop, cm_val], class_weight = class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(x_test)\n",
    "onehot_preds = np.zeros((x_test.shape[0], 6))\n",
    "i = 0 \n",
    "for pred in preds:\n",
    "    onehot_preds[i][np.argmax(preds[i])] = 1\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cmatrix = confusion_matrix(y_test.argmax(axis=1), onehot_preds.argmax(axis=1))\n",
    "\n",
    "print(cmatrix)\n",
    "\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_cm = pd.DataFrame(cmatrix, range(5), range(5))\n",
    "# plt.figure(figsize=(10,7))\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "\n",
    "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}) # font size\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
